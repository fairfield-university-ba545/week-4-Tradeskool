{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and Engineering - Beginner's Guide\n",
    "\n",
    "Learn how feature engineering can help you to up your game when building machine learning models in Kaggle: create new columns, transform variables and more!\n",
    "\n",
    "In the previous classes, you have already learned about Exploratory Data Analysis (EDA) and building simple machine learning models. In this class, you'll learn more about feature engineering, a process where you use domain knowledge of your data to create additional relevant features that increase the predictive power of the learning algorithm and make your machine learning models perform even better!\n",
    "\n",
    "More specifically,\n",
    "\n",
    "- You'll first get started by doing all necessary imports and getting the data in your workspace;\n",
    "- Then, you'll see some reasons why you should do feature engineering and start working on engineering your own new features for your data set! You'll create new columns, transform variables into numerical ones, handle missing values, and much more.\n",
    "- Lastly, you'll build a new machine learning model with your new data set and submit it to Kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Started!\n",
    "\n",
    "Before you can start off, you're going to do all the imports, just like you did in the previous tutorial, use some IPython magic to make sure the figures are generated inline in the Jupyter Notebook and set the visualization style. Next, you can import your data and make sure that you store the target variable of the training data in a safe place. Afterwards, you merge the train and test data sets (with exception of the `'Survived'` column of `df_train`) and store the result in `data`.\n",
    "\n",
    "Remember that you do this because you want to make sure that any preprocessing that you do on the data is reflected in both the train and test sets!\n",
    "\n",
    "Lastly, you use the `.info()` method to take a look at your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Figures inline and set visualization style\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store target variable of training data in a safe place\n",
    "survived_train = df_train.Survived\n",
    "\n",
    "# Concatenate training and test sets\n",
    "data = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n",
    "\n",
    "# View head\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Feature Engineering At All?\n",
    "You perform feature engineering to extract more information from your data, so that you can up your game when building models.\n",
    "\n",
    "## Titanic's Passenger Titles\n",
    "Let's check out what this is all about by looking at an example. Let's check out the `'Name'` column with the help of the `.tail()` method, which helps you to see the last five rows of your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View head of 'Name' column\n",
    "data.Name.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suddenly, you see different titles emerging! In other words, this column contains strings or text that contain titles, such as 'Mr', 'Master' and 'Dona'.\n",
    "\n",
    "These titles of course give you information on social status, profession, etc., which in the end could tell you something more about survival.\n",
    "\n",
    "At first sight, it might seem like a difficult task to separate the names from the titles, but don't panic! Remember, you can easily use regular expressions to extract the title and store it in a new column `'Title'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Title from Name, store in column and plot barplot\n",
    "data['Title'] = data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n",
    "sns.countplot(x='Title', data=data);\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this new column 'Title' is actually a new feature for your data set!\n",
    "\n",
    "Tip: to learn more about regular expressions, check out my write up of our last [FB Live code along event](https://www.datacamp.com/community/tutorials/web-scraping-python-nlp) or check out DataCamp's [Python Regular Expressions Tutorial](https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial).\n",
    "\n",
    "You can see that there are several titles in the above plot and there are many that don't occur so often. So, it makes sense to put them in fewer buckets.\n",
    "\n",
    "For example, you probably want to replace `'Mlle'` and `'Ms'` with `'Miss'` and `'Mme'` by `'Mrs'`, as these are French titles and ideally, you want all your data to be in one language. Next, you also take a bunch of titles that you can't immediately categorize and put them in a bucket called `'Special'`.\n",
    "\n",
    "Tip: play around with this to see how your algorithm performs as a function of it!\n",
    "\n",
    "Next, you view a barplot of the result with the help of the `.countplot()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Title'] = data['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})\n",
    "data['Title'] = data['Title'].replace(['Don', 'Dona', 'Rev', 'Dr',\n",
    "                                            'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'],'Special')\n",
    "sns.countplot(x='Title', data=data);\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what your newly engineered feature `'Title'` looks like!\n",
    "\n",
    "Now, make sure that you have a `'Title'` column and check out your data again with the `.tail()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View head of data\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passenger's Cabins\n",
    "When you loaded in the data and inspected it, you saw that there are several `NaN`s or missing values in the `'Cabin'` column.\n",
    "\n",
    "It is reasonable to presume that those NaNs didn't have a cabin, which could tell you something about `'Survival'`. So, let's now create a new column `'Has_Cabin'` that encodes this information and tells you whether passengers had a cabin or not.\n",
    "\n",
    "Note that you use the `.isnull()` method in the code chunk below, which will return `True` if the passenger doesn't have a cabin and `False` if that's not the case. However, since you want to store the result in a column `'Has_Cabin'`, you actually want to flip the result: you want to return `True` if the passenger has a cabin. That's why you use the tilde `~`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Did they have a Cabin?\n",
    "data['Has_Cabin'] = ~data.Cabin.isnull()\n",
    "\n",
    "# View head of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you want to do now is drop a bunch of columns that contain no more useful information (or that we're not sure what to do with). In this case, you're looking at columns such as `['Cabin', 'Name', 'PassengerId', 'Ticket']`, because\n",
    "\n",
    "- You already extracted information on whether or not the passenger had a cabin in your newly added 'Has_Cabin' column;\n",
    "- Also, you already extracted the titles from the 'Name' column;\n",
    "- You also drop the 'PassengerId' and the 'Ticket' columns because these will probably not tell you anything more about the survival of the Titanic passengers.\n",
    "\n",
    "Tip there might be more information in the 'Cabin' column, but for this tutorial, you assume that there isn't!\n",
    "\n",
    "To drop these columns in your actual data DataFrame, make sure to use the `inplace` argument in the `.drop()` method and set it to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop columns and view head\n",
    "data.drop(['Cabin', 'Name', 'PassengerId', 'Ticket'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You've successfully engineered some new features such as `'Title'` and `'Has_Cabin'` and made sure that features that don't add any more useful information for your machine learning model are now dropped from your DataFrame!\n",
    "\n",
    "Next, you want to deal with deal with missing values, bin your numerical data, and transform all features into numeric variables using `.get_dummies()` again. Lastly, you'll build your final model for this tutorial. Check out how all of this is done in the next sections!\n",
    "\n",
    "## Handling Missing Values\n",
    "With all of the changes you have made to your original data DataFrame, it's a good idea to figure out if there are any missing values left with `.info()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the above line of code tells you that you have missing values in `'Age'`, `'Fare'`, and `'Embarked'`.\n",
    "\n",
    "Remember that you can easily spot this by first looking at the total number of entries (1309) and then checking out the number of non-null values in the columns that `.info()` lists. In this case, you see that `'Age'` has 1046 non-null values, so that means that you have 263 missing values. Similarly, `'Fare'` only has one missing value and 'Embarked' has two missing values.\n",
    "\n",
    "Just like you did in the previous tutorial, you're going to impute these missing values with the help of `.fillna()`:\n",
    "\n",
    "Note that, once again, you use the median to fill in the `'Age'` and `'Fare'` columns because it's perfect for dealing with outliers. Other ways to impute missing values would be to use the mean, which you can find by adding all data points and dividing by the number of data points, or mode, which is the number that occurs the highest number of times.\n",
    "\n",
    "You fill in the two missing values in the `'Embarked'` column with `'S'`, which stands for Southampton, because this value is the most common one out of all the values that you find in this column.\n",
    "\n",
    "Tip: you can double check this by doing some more Exploratory Data Analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Impute missing values for Age, Fare, Embarked\n",
    "data['Age'] = data.Age.fillna(data.Age.median())\n",
    "data['Fare'] = data.Fare.fillna(data.Fare.median())\n",
    "data['Embarked'] = data['Embarked'].fillna('S')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin numerical data\n",
    "Next, you want to bin the numerical data, because you have a range of ages and fares. However, there might be fluctuations in those numbers that don't reflect patterns in the data, which might be noise. That's why you'll put people that are within a certain range of age or fare in the same bin. You can do this by using the `pandas` function `qcut()` to bin your numerical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Binning numerical columns\n",
    "data['CatAge'] = pd.qcut(data.Age, q=4, labels=False )\n",
    "data['CatFare']= pd.qcut(data.Fare, q=4, labels=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you pass in the data as a Series, `data.Age` and `data.Fare`, after which you specify the number of quantiles, `q=4`. Lastly, you set the `labels` argument to `False` to encode the bins as numbers.\n",
    "\n",
    "Now that you have all of that information in bins, you can now safely drop `'Age'` and `'Fare'` columns. Don't forget to check out the first five rows of your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.drop(['Age', 'Fare'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Members in Family Onboard\n",
    "The next thing you can do is create a new column, which is the number of members in families that were onboard of the Titanic. In this tutorial, you won't go in this and see how the model performs without it. If you do want to check out how the model would do with this additional column, run the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create column of number of Family members onboard\n",
    "data['Fam_Size'] = data.Parch + data.SibSp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, you will just go ahead and drop the `'SibSp'` and `'Parch'` columns from your DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "data = data.drop(['SibSp','Parch'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have you noticed?\n",
    "\n",
    "Have you noticed that we have completed several feature engineering steps? From **binning numerical variables** to figure out **number of family members onboard**, these are both feature engineering steps. So essentially, we are creating new features based on existing features - using **background human intelligence** - and then remove the original variable(s). This requires you as an analyst to:\n",
    "\n",
    "- Understand the dataset, in particular the data dictionary well;\n",
    "- Understand the business background well.\n",
    "\n",
    "It is easy said than done - but that would be the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Variables into Numerical Variables\n",
    "Now that you have engineered some more features, such as `'Title'` and `'Has_Cabin'`, and you have dealt with missing values, binned your numerical data, it's time to transform all variables into numeric ones. You do this because machine learning models generally take numeric input.\n",
    "\n",
    "As you have done previously, you will use `.get_dummies()` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform into binary variables\n",
    "data_dum = pd.get_dummies(data, drop_first=True)\n",
    "data_dum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of this done, it's time to build your final model!\n",
    "\n",
    "\n",
    "## Building models with Your New Data Set!\n",
    "As before, you'll first split your `data` back into training and test sets. Then, you'll transform them into arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into test.train\n",
    "data_train = data_dum.iloc[:891]\n",
    "data_test = data_dum.iloc[891:]\n",
    "\n",
    "# Transform into arrays for scikit-learn\n",
    "X = data_train.values\n",
    "test = data_test.values\n",
    "y = survived_train.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now going to build a decision tree on your brand new feature-engineered dataset. To choose your hyperparameter `max_depth`, you'll use a variation on test train split called **cross validation**.\n",
    "<img src='http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1514303215/cv_raxrt7.png' />\n",
    "\n",
    "You begin by splitting the dataset into `5` groups or folds. Then you hold out the *first* fold as a **test set**, fit your model on the remaining four folds, predict on the test set and compute the metric of interest. Next, you hold out the *second* fold as your **test set**, fit on the remaining data, predict on the test set and compute the metric of interest. Then similarly with the *third*, *fourth* and *fifth*.\n",
    "\n",
    "**NOTE**: Cross-validation (CV) is an efficient way of avoid overfitting your models - be sure to use it when possible.\n",
    "\n",
    "As a result, you get five values of accuracy, from which you can compute statistics of interest, such as the median and/or mean and 95% confidence intervals.\n",
    "\n",
    "You do this for each value of each hyperparameter that you're tuning and choose the set of hyperparameters that performs the best. This is called **grid search**.\n",
    "\n",
    "Enough about that for now, let's get it!\n",
    "\n",
    "In the following, you'll use **cross validation** and **grid search** to choose the best `max_depth` for your new feature-engineered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup the hyperparameter grid\n",
    "dep = np.arange(1,9)\n",
    "param_grid = {'max_depth' : dep}\n",
    "\n",
    "# Instantiate a decision tree classifier: clf\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the GridSearchCV object: clf_cv\n",
    "clf_cv = GridSearchCV(clf, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "clf_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameter and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(clf_cv.best_params_))\n",
    "print(\"Best score is {}\".format(clf_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can make predictions on your test set, create a new column `'Survived'` and store your predictions in it. Don't forget to save the `'PassengerId'` and `'Survived'` columns of `df_test` to a .csv and submit it to Kaggle to look at how good your model is.\n",
    "The accuracy of your submission is 78.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
